{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpcJsqajcua6"
      },
      "source": [
        "李萌萌的电子骨灰盒\n",
        "----\n",
        "\n",
        "这是一个通过ChatGLM模型训练的李萌萌的数字分身，你可以在问题栏目填入内容，或者什么都不填，来观察李萌萌到底会说些什么。\n",
        "T4级别的GPU已经可以很胜任这个任务了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRViHJfokAHa"
      },
      "outputs": [],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep-GXSbvc4UR"
      },
      "source": [
        "### 安装依赖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG5LbTdJcr-a"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/ljsabc/Fujisaki\n",
        "%cd Fujisaki\n",
        "\n",
        "%pip install -q -r requirements.txt\n",
        "%pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRnutF1NeiEh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "from transformers import AutoTokenizer, GenerationConfig, AutoModel\n",
        "\n",
        "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", revision=\"658202d\", trust_remote_code=True).half().cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, revision=\"658202d\")\n",
        "setattr(model, \"lm_head_raw\", model.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGcANpbi1k4z"
      },
      "source": [
        "The huggingface repo has something more than the QKV lora. We checkout a version that is the closest to the current version. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggFUQpf2en_s"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "peft_path = 'ljsabc/Fujisaki_GLM'      # change it to your own\n",
        "model = PeftModel.from_pretrained(\n",
        "       model,\n",
        "       peft_path,\n",
        "       torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "# We have to use full precision, as some tokens are >65535\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wv8zzKtgre2"
      },
      "outputs": [],
      "source": [
        "def evaluate(context, temperature, top_p, top_k):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        #repetition_penalty=1.1,\n",
        "        num_beams=1,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        input_text = f\"Context: {context}Answer: \" \n",
        "        ids = tokenizer([input_text], return_tensors=\"pt\")\n",
        "        inputs = ids.to(\"cuda\")\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_length=224,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        out = out.tolist()[0]\n",
        "        decoder_output = tokenizer.decode(out)\n",
        "        out_text = decoder_output.split(\"Answer: \")[1]\n",
        "        return out_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7PNWg3gFRJw"
      },
      "outputs": [],
      "source": [
        "def evaluate_stream(msg, history, temperature, top_p):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        #repetition_penalty=1.1,\n",
        "        num_beams=1,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    history.append([msg, None])\n",
        "\n",
        "    context = \"\"\n",
        "    if len(history) > 4:\n",
        "        history.pop(0)\n",
        "\n",
        "    for j in range(len(history)):\n",
        "        history[j][0] = history[j][0].replace(\"<br>\", \"\")\n",
        "\n",
        "    # concatenate context\n",
        "    for h in history[:-1]:\n",
        "        context += h[0] + \"||\" + h[1] + \"||\"\n",
        "\n",
        "    context += history[-1][0]\n",
        "    context = context.replace(r'<br>', '')\n",
        "\n",
        "    # TODO: Avoid the tokens are too long.\n",
        "    CUTOFF = 224\n",
        "    while len(tokenizer.encode(context)) > CUTOFF:\n",
        "        # save 15 token size for the answer\n",
        "        context = context[15:]\n",
        "\n",
        "    h = []\n",
        "    print(\"History:\", history)\n",
        "    print(\"Context:\", context)\n",
        "    for response, h in model.stream_chat(tokenizer, context, h, max_length=CUTOFF, top_p=top_p, temperature=temperature):\n",
        "        history[-1][1] = response\n",
        "        yield history, \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egPtBitEHsz1"
      },
      "outputs": [],
      "source": [
        "history = [['你是谁','我是喵喵'], ['你住在哪里', \"我不知道\"]]\n",
        "for h in evaluate_stream(\"你在干什么\", history, 1.0, 0.9):\n",
        "    print(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xy1tvCTApYN"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "title = \"\"\"<h1 align=\"center\">李萌萌（Alter Ego）</h1>\n",
        "<h3 align=\"center\">这是一个通过ChatGLM模型训练的李萌萌的数字分身，你可以与她聊天，或者直接在文本框按下Enter，来观察李萌萌到底会说些什么。</h3>\"\"\"\n",
        "\n",
        "footer =  \"\"\"<p align='center'>项目在<a href='https://github.com/ljsabc/Fujisaki' target='_blank'>GitHub</a>上托管，基于清华的<a href='https://huggingface.co/THUDM/chatglm-6b' target='_blank'>THUDM/chatglm-6b</a>项目。</p>\n",
        "<p align='center'><em>\"I'm... a boy.\" --Chihiro Fujisaki</em></p>\"\"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.HTML(title)\n",
        "    state = gr.State()\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            temp = gr.components.Slider(minimum=0, maximum=1.1, value=0.95, label=\"Temperature\",\n",
        "                info=\"温度参数，越高的温度生成的内容越丰富，但是有可能出现语法问题。\")\n",
        "            top_p = gr.components.Slider(minimum=0.5, maximum=1.0, value=0.99, label=\"Top-p\",\n",
        "                info=\"top-p参数，只输出前p>top-p的文字，越大生成的内容越丰富，但也可能出现语法问题。数字越小似乎上下文的衔接性越好。\")\n",
        "            #code = gr.Textbox(label=\"temp_output\", info=\"解码器输出\")\n",
        "            #top_k = gr.components.Slider(minimum=1, maximum=200, step=1, value=25, label=\"Top k\",\n",
        "            #    info=\"top-k参数，下一个输出的文字会从top-k个文字中进行选择，越大生成的内容越丰富，但也可能出现语法问题。数字越小似乎上下文的衔接性越好。\")\n",
        "            \n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(label=\"聊天框\", info=\"\")\n",
        "            msg = gr.Textbox(label=\"输入框\", placeholder=\"最近过得怎么样？\",\n",
        "                info=\"输入你的内容，按[Enter]发送。也可以什么都不填写生成随机数据。\")\n",
        "            clear = gr.Button(\"清除聊天\")\n",
        "\n",
        "    msg.submit(evaluate_stream, [msg, chatbot, temp, top_p], [chatbot, msg])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "    gr.HTML(footer)\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(debug=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsS8ZYH5ficF"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "gr.Interface(\n",
        "    fn=evaluate,\n",
        "    inputs=[\n",
        "        gr.components.Textbox(\n",
        "            lines=2, label=\"问题\", placeholder=\"最近过得怎么样？\",\n",
        "            info=\"可以在这里输入你的问题。也可以什么都不填写生成随机数据。\"\n",
        "        ),\n",
        "        #gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n",
        "        gr.components.Slider(minimum=0, maximum=1.1, value=1.0, label=\"Temperature\",\n",
        "            info=\"温度参数，越高的温度生成的内容越丰富，但是有可能出现语法问题。\"),\n",
        "        gr.components.Slider(minimum=0.5, maximum=1.0, value=0.99, label=\"Top p\",\n",
        "            info=\"top-p参数，只输出前p>top-p的文字，建议不要修改。\"),\n",
        "        gr.components.Slider(minimum=1, maximum=200, step=1, value=25, label=\"Top k\",\n",
        "            info=\"top-k参数，下一个输出的文字会从top-k个文字中进行选择，越大生成的内容越丰富，但也可能出现语法问题。数字越小似乎上下文的衔接性越好。\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.inputs.Textbox(\n",
        "            lines=5,\n",
        "            label=\"Output\",\n",
        "        )\n",
        "    ],\n",
        "    title=\"李萌萌（Alter Ego）\",\n",
        "    description=\"这是一个通过ChatGLM模型训练的李萌萌的数字分身，你可以在问题栏目填入内容，或者什么都不填，来观察李萌萌到底会说些什么。\",\n",
        ").launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
